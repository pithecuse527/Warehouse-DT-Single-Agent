{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5369b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from mlagents.envs import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad05d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = [84, 84, 3]\n",
    "action_size = 8\n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 32\n",
    "mem_maxlen = 50000\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.00025\n",
    "\n",
    "run_episode = 15000\n",
    "test_episode = 1000\n",
    "\n",
    "start_train_episode = 1000\n",
    "\n",
    "target_update_step = 10000\n",
    "print_interval = 100\n",
    "save_interval = 5000\n",
    "\n",
    "epsilon_init = 1.0\n",
    "epsilon_min = 0.1\n",
    "\n",
    "# environment setting\n",
    "env_config = {\"gridSize\": 7, \"numGoals\": 2, \"numBoxes\": 2, \"numObstacles\":1}\n",
    "\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "# environment path\n",
    "game = \"single-agent\"\n",
    "env_name = \"../env/\" + game\n",
    "\n",
    "# save and load models path\n",
    "save_path = \"../saved_models/\" + game + \"/\" + date_time + \"_DQN\"\n",
    "# load_path = \"../saved_models/\" + game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0736a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_name):\n",
    "        self.input = tf.placeholder(shape=[None, state_size[0], state_size[1], \n",
    "                                           state_size[2]], dtype=tf.float32)\n",
    "        # normalize pixels into 0 ~ 1\n",
    "        self.input_normalize = (self.input - (255.0 / 2)) / (255.0 / 2)\n",
    "\n",
    "        # CNN Network -> Three convolution layers and two fully connected layers\n",
    "        with tf.variable_scope(name_or_scope=model_name):\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.input_normalize, filters=32, \n",
    "                                          activation=tf.nn.relu, kernel_size=[8,8], \n",
    "                                          strides=[4,4], padding=\"SAME\")\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, \n",
    "                                          activation=tf.nn.relu, kernel_size=[4,4],\n",
    "                                          strides=[2,2],padding=\"SAME\")\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, \n",
    "                                          activation=tf.nn.relu, kernel_size=[3,3],\n",
    "                                          strides=[1,1],padding=\"SAME\")\n",
    " \n",
    "            self.flat = tf.layers.flatten(self.conv3)\n",
    "\n",
    "            self.fc1 = tf.layers.dense(self.flat,512,activation=tf.nn.relu)\n",
    "            self.Q_Out = tf.layers.dense(self.fc1, action_size, activation=None)\n",
    "        self.predict = tf.argmax(self.Q_Out, 1)\n",
    "\n",
    "        self.target_Q = tf.placeholder(shape=[None, action_size], dtype=tf.float32)\n",
    "\n",
    "        # calculate loss and optimize\n",
    "        self.loss = tf.losses.huber_loss(self.target_Q, self.Q_Out)\n",
    "        self.UpdateModel = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.trainable_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c3c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = Model(\"Q\")  # normal model\n",
    "        self.target_model = Model(\"target\")  # target model\n",
    "\n",
    "        self.memory = deque(maxlen=mem_maxlen)  # replay memory\n",
    "   \n",
    "        self.sess = tf.Session()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "\n",
    "        self.Saver = tf.train.Saver()\n",
    "        self.Summary, self.Merge = self.Make_Summary()\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "        if load_model == True:\n",
    "            self.Saver.restore(self.sess, load_path)\n",
    "\n",
    "    # epsilon greedy\n",
    "    def get_action(self, state):\n",
    "        if self.epsilon > np.random.rand():\n",
    "            return np.random.randint(0, action_size)\n",
    "        else:\n",
    "            predict = self.sess.run(self.model.predict, feed_dict={self.model.input: state})\n",
    "            return np.asscalar(predict)\n",
    "\n",
    "    # add data to replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state[0], action, reward, next_state[0], done))\n",
    "\n",
    "    # save network model \n",
    "    def save_model(self):\n",
    "        self.Saver.save(self.sess, save_path + \"/model/model\")\n",
    "\n",
    "    # training\n",
    "    def train_model(self, done):\n",
    "        # decrease epsilon\n",
    "        if done:\n",
    "            if self.epsilon > epsilon_min:\n",
    "                self.epsilon -= 0.0004\n",
    "\n",
    "        # mini batch sampling for training\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            states.append(mini_batch[i][0])\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states.append(mini_batch[i][3])\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        # get target value\n",
    "        target = self.sess.run(self.model.Q_Out, feed_dict={self.model.input: states})\n",
    "        target_val = self.sess.run(self.target_model.Q_Out, \n",
    "                                   feed_dict={self.target_model.input: next_states})\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + discount_factor * np.amax(target_val[i])\n",
    "\n",
    "        # do training and calculate loss\n",
    "        _, loss = self.sess.run([self.model.UpdateModel, self.model.loss],\n",
    "                                feed_dict={self.model.input: states, \n",
    "                                           self.model.target_Q: target})\n",
    "        return loss\n",
    "\n",
    "    # target network update\n",
    "    def update_target(self):\n",
    "        for i in range(len(self.model.trainable_var)):\n",
    "            self.sess.run(self.target_model.trainable_var[i].assign(self.model.trainable_var[i]))\n",
    "\n",
    "    # write summaries to Tensorboard\n",
    "    def Make_Summary(self):\n",
    "        self.summary_loss = tf.placeholder(dtype=tf.float32)\n",
    "        self.summary_reward = tf.placeholder(dtype=tf.float32)\n",
    "        tf.summary.scalar(\"loss\", self.summary_loss)\n",
    "        tf.summary.scalar(\"reward\", self.summary_reward)\n",
    "        Summary = tf.summary.FileWriter(logdir=save_path, graph=self.sess.graph)\n",
    "        Merge = tf.summary.merge_all()\n",
    "\n",
    "        return Summary, Merge\n",
    "    \n",
    "    def Write_Summray(self, reward, loss, episode):\n",
    "        self.Summary.add_summary(\n",
    "            self.sess.run(self.Merge, feed_dict={self.summary_loss: loss, \n",
    "                                                 self.summary_reward: reward}), episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c953763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'SokobanAcademy' started successfully!\n",
      "Unity Academy name: SokobanAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\tnumBoxes -> 2.0\n",
      "\t\tnumGoals -> 2.0\n",
      "\t\tnumObstacles -> 1.0\n",
      "\t\tgridSize -> 7.0\n",
      "Unity brain name: SokobanLearning\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [8]\n",
      "        Vector Action descriptions: \n",
      "INFO:mlagents.envs:Academy reset with parameters: gridSize -> 7, numGoals -> 2, numBoxes -> 2, numObstacles -> 1\n",
      "C:\\Users\\ji\\anaconda3\\envs\\tf12_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\ji\\anaconda3\\envs\\tf12_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7229 / episode: 100 / reward: -1.11 / loss: nan / epsilon: 1.000\n",
      "step: 15292 / episode: 200 / reward: -1.27 / loss: nan / epsilon: 1.000\n",
      "step: 20187 / episode: 300 / reward: -1.13 / loss: nan / epsilon: 1.000\n",
      "step: 27752 / episode: 400 / reward: -1.06 / loss: nan / epsilon: 1.000\n",
      "step: 34587 / episode: 500 / reward: -1.18 / loss: nan / epsilon: 1.000\n",
      "step: 40540 / episode: 600 / reward: -1.20 / loss: nan / epsilon: 1.000\n",
      "step: 48176 / episode: 700 / reward: -1.00 / loss: nan / epsilon: 1.000\n",
      "step: 54312 / episode: 800 / reward: -1.06 / loss: nan / epsilon: 1.000\n",
      "step: 60949 / episode: 900 / reward: -1.19 / loss: nan / epsilon: 1.000\n",
      "step: 67274 / episode: 1000 / reward: -1.15 / loss: nan / epsilon: 1.000\n",
      "step: 75799 / episode: 1100 / reward: -1.18 / loss: 0.0009 / epsilon: 0.960\n",
      "step: 82868 / episode: 1200 / reward: -1.14 / loss: 0.0004 / epsilon: 0.920\n",
      "step: 90160 / episode: 1300 / reward: -1.06 / loss: 0.0003 / epsilon: 0.880\n",
      "step: 97095 / episode: 1400 / reward: -0.85 / loss: 0.0004 / epsilon: 0.840\n",
      "step: 103614 / episode: 1500 / reward: -0.94 / loss: 0.0004 / epsilon: 0.800\n",
      "step: 109262 / episode: 1600 / reward: -0.94 / loss: 0.0003 / epsilon: 0.760\n",
      "step: 115102 / episode: 1700 / reward: -1.02 / loss: 0.0004 / epsilon: 0.720\n",
      "step: 121417 / episode: 1800 / reward: -0.98 / loss: 0.0004 / epsilon: 0.680\n",
      "step: 127771 / episode: 1900 / reward: -0.82 / loss: 0.0004 / epsilon: 0.640\n",
      "step: 134055 / episode: 2000 / reward: -0.87 / loss: 0.0004 / epsilon: 0.600\n",
      "step: 140608 / episode: 2100 / reward: -0.82 / loss: 0.0004 / epsilon: 0.560\n",
      "step: 147496 / episode: 2200 / reward: -0.77 / loss: 0.0004 / epsilon: 0.520\n",
      "step: 154638 / episode: 2300 / reward: -0.97 / loss: 0.0004 / epsilon: 0.480\n",
      "step: 161284 / episode: 2400 / reward: -0.79 / loss: 0.0004 / epsilon: 0.440\n",
      "step: 168132 / episode: 2500 / reward: -0.72 / loss: 0.0004 / epsilon: 0.400\n",
      "step: 175398 / episode: 2600 / reward: -1.03 / loss: 0.0004 / epsilon: 0.360\n",
      "step: 184015 / episode: 2700 / reward: -0.93 / loss: 0.0004 / epsilon: 0.320\n",
      "step: 192404 / episode: 2800 / reward: -0.80 / loss: 0.0003 / epsilon: 0.280\n",
      "step: 201006 / episode: 2900 / reward: -0.75 / loss: 0.0003 / epsilon: 0.240\n",
      "step: 209199 / episode: 3000 / reward: -0.53 / loss: 0.0003 / epsilon: 0.200\n",
      "step: 218377 / episode: 3100 / reward: -0.69 / loss: 0.0003 / epsilon: 0.160\n",
      "step: 228762 / episode: 3200 / reward: -0.54 / loss: 0.0002 / epsilon: 0.120\n",
      "step: 242416 / episode: 3300 / reward: -0.99 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 254791 / episode: 3400 / reward: -1.14 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 267258 / episode: 3500 / reward: -0.82 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 279105 / episode: 3600 / reward: -0.73 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 291272 / episode: 3700 / reward: -0.60 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 304326 / episode: 3800 / reward: -0.39 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 316633 / episode: 3900 / reward: -0.85 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 328886 / episode: 4000 / reward: -0.92 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 341882 / episode: 4100 / reward: -0.64 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 354257 / episode: 4200 / reward: -0.87 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 367760 / episode: 4300 / reward: -0.60 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 380409 / episode: 4400 / reward: -0.20 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 394440 / episode: 4500 / reward: -0.45 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 407402 / episode: 4600 / reward: -0.42 / loss: 0.0001 / epsilon: 0.100\n",
      "step: 418765 / episode: 4700 / reward: 0.05 / loss: 0.0002 / epsilon: 0.100\n",
      "step: 429796 / episode: 4800 / reward: -0.13 / loss: 0.0002 / epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # create unity environment\n",
    "    env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "    # set unity brain\n",
    "    default_brain = env.brain_names[0]\n",
    "    brain = env.brains[default_brain]\n",
    "\n",
    "    # create agent\n",
    "    agent = DQNAgent()\n",
    "\n",
    "    step = 0\n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "    # environment setting\n",
    "    env_info = env.reset(train_mode=train_mode, config=env_config)[default_brain]\n",
    "\n",
    "    # run episodes \n",
    "    for episode in range(run_episode + test_episode):\n",
    "        if episode > run_episode:\n",
    "            train_mode = False\n",
    "            env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "        \n",
    "        # init. state, episode_rewards, done\n",
    "        state = np.uint8(255 * np.array(env_info.visual_observations[0]))\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "\n",
    "        # one episode\n",
    "        while not done:\n",
    "            step += 1\n",
    "\n",
    "            # get action and let the agent take the action\n",
    "            action = agent.get_action(state)\n",
    "            env_info = env.step(action)[default_brain]\n",
    "\n",
    "            # get information of the next state\n",
    "            next_state = np.uint8(255 * np.array(env_info.visual_observations[0]))\n",
    "            reward = env_info.rewards[0]\n",
    "            episode_rewards += reward\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            # add data to replay memroy\n",
    "            if train_mode:\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "            else:\n",
    "                time.sleep(0.01) \n",
    "                agent.epsilon = 0.05\n",
    "\n",
    "            # state update\n",
    "            state = next_state\n",
    "\n",
    "            if episode > start_train_episode and train_mode:\n",
    "                # training\n",
    "                loss = agent.train_model(done)\n",
    "                losses.append(loss)\n",
    "\n",
    "                # target network update at a certain step\n",
    "                if step % (target_update_step) == 0:\n",
    "                    agent.update_target()\n",
    "\n",
    "        rewards.append(episode_rewards)\n",
    "\n",
    "        # print and write episode information\n",
    "        if episode % print_interval == 0 and episode != 0:\n",
    "            print(\"step: {} / episode: {} / reward: {:.2f} / loss: {:.4f} / epsilon: {:.3f}\".format\n",
    "                  (step, episode, np.mean(rewards), np.mean(losses), agent.epsilon))\n",
    "            agent.Write_Summray(np.mean(rewards), np.mean(losses), episode)\n",
    "            rewards = []\n",
    "            losses = []\n",
    "\n",
    "        # save network model\n",
    "        if episode % save_interval == 0 and episode != 0:\n",
    "            agent.save_model()\n",
    "            print(\"Save Model {}\".format(episode))\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2da3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
